
RIDGE REGRESSION:This is also called as Tikhonov Regularization.
This is standard regression models that an individual can avail to analyze the data.
In this the cost function is altered by adding a penalty equivalent to square of the magnitiude of the coefficients.
Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity.
It is used to create a parsimonious model.
It uses L2 regularization.

LASSO REGRESSION:It is the type of linear regression that uses shrinkages.
'LASSO' stands for Least Absolute Shrinkage and Selector Operator.
The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable.
The lasso does this imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink towards zero.
It uses L1 penalty term

ELASTIC NET REGRESSION:Elatic net incorporates penalities from both L1 and L2 regularization.
It is always peferred over lasso and ridge regression because it solves the limitations of both methods,while also including each as special cases.
It first emerged as result of critique on lasso,whose variable selection can be too dependent on data and thus unstale.
It encourages group effect in case of highly correlated variables.
There are no limitations on the number of selected variables.
